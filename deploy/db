#!/bin/bash

# exit when any command fails
set -e
# enable !! command completion
# set -o history -o histexpand

describe_length() {
  local num="$1"
  local max="$2"
  if [ ${num} -gt ${max} ]; then
    echo "First ${max} of ${num}"
  else
    echo "All"
  fi
}

mail_developers(){
  local developers_emails=$DB_DEPLOY_EMAILS
  if [ -z "$developers_emails" ]; then
    echo "No developers emails specified. Please set DB_DEPLOY_EMAILS in your .env file."
    exit 1
  fi
  local subject=$1
  local body_prefix=$2

  if [ ! -z $LOG_FILE ]; then
    local num_errors=$(grep ERROR ${LOG_FILE} | wc -l)
    local max_errors=100
    local error_desc="$(describe_length ${num_errors} ${max_errors})"
    local errors_snippet="$(grep ERROR ${LOG_FILE} | head -${max_errors})"

    local num_log_lines=$(cat ${LOG_FILE} | wc -l)
    local max_log_lines=100
    local log_desc="$(describe_length ${num_log_lines} ${max_log_lines})"
    local log_snippet=$(tail -${max_log_lines} "${LOG_FILE}")

    local body=$(cat <<END_HEREDOC
${body_prefix}

log file: ${LOG_FILE}

----------

${error_desc} error messages in the log:

${errors_snippet}

----------

${log_desc} lines from the log:

${log_snippet}
END_HEREDOC
)
  fi

  echo -e "$body" | mail -s "[sd-graph deploy/db] $subject" $developers_emails
}

on_exit(){
  local last_exit_code=$1
  local line_number=$last_line
  local last_cmd=$current_command
  if [ $last_exit_code -eq 0 ]; then
    mail_developers "$STAGE_NAME $PERIODICITY: success" "$last_cmd exited with code $last_exit_code"
  else
    mail_developers "$STAGE_NAME $PERIODICITY: errors" "$last_cmd exited with code $last_exit_code on line $line_number"
  fi
}

# https://medium.com/@dirk.avery/the-bash-trap-trap-ce6083f36700
# keep track of the last executed command
trap 'last_command=$current_command; current_command=$BASH_COMMAND; last_line=$LINENO' DEBUG
trap 'on_exit $?' EXIT


log(){
  local timestamp=$(date +%Y-%m-%d\ %H:%M:%S)
  echo "[deploy/db $STAGE_NAME $PERIODICITY $timestamp] $1"
}

load_dotenv(){
  if [ -f .env ]
  then
    log "loading .env file"
    # allexport automatically exports all variables that are assigned, i.e. `FOO=bar` would be equivalent to `export FOO=bar`.
    # We temporarily enable it for loading the .env file to export all the variables in there.
    set -o allexport
    source .env
    set +o allexport
  else
    log ".env file missing. Aborting"
    exit 1
  fi
}

eeb() {
  docker-compose -f docker-compose.yml -f docker-compose.deploy.yml $@
}

update_local_neo4j() {
  reset_indices() {
    # RUN THIS ONLY MANUALLY
    # remove indices from automatic steps
    log "defining indices"
    cat sdg/SD-indices.cql | eeb run --rm neo4j cypher-shell -a bolt://neo4j:7687 -u neo4j -p $NEO_PASSWORD
  }

  update_open() {
    # always start with this when updating db
    log "opening the update"
    cat sdg/update_open.cql | eeb run --rm neo4j cypher-shell -a bolt://neo4j:7687 -u neo4j -p $NEO_PASSWORD
  }

  update_fulltext_biorxiv() {
    log "updating meca archives"
    # only downloads MECA archives from 2021 onwards
    aws s3 sync \
      --request-payer requester \
      --exclude "*" --include "*.meca" \
      --exclude "*_2018/*" --exclude "*_2019/*" --exclude "*_2020/*" --exclude "*_2021/*" --exclude "*_2022/*" --exclude "*_2023/*" \
      s3://biorxiv-src-monthly/Current_Content $BIORXIV_PATH

    log "remove prelim articles obtained from the CrossRef and bioRxiv APIs"
    cat neotools/purge_prelim.cql | eeb run --rm neo4j cypher-shell -a bolt://neo4j:7687 -u neo4j -p $NEO_PASSWORD

    log "import full text biorxiv preprints"
    biorxiv_folder_name() {
      local month_delta="$1"
      local MONTH=`LC_TIME=en_US.UTF-8 date --date="-$month_delta month" +%B`
      local YEAR=`date --date="-$month_delta month" +%Y`
      echo "${MONTH}_${YEAR}"
    }
    # Import everything from the current and the previous month to ensure we get all preprints.
    local delta_this_month=0
    local delta_last_month=1

    for delta in "${delta_last_month}" "${delta_this_month}"; do
      folder_name=`biorxiv_folder_name "${delta}"`
      log "importing full text biorxiv preprints from ${BIORXIV_PATH}/${folder_name}"
      eeb run --rm flask python -m neotools.rxiv2neo "/app/biorxiv/$folder_name" --type meca
    done
  }

  import_peer_reviews() {
    # daily
    log "import peer reviews from peer-review databases & complete our db with biorxiv prelim api if our meca-fulltext-db missed one"
    eeb run --rm flask python -m peerreview.neohypo hypothesis
    eeb run --rm flask python -m peerreview.neohypo rrc19
    eeb run --rm flask python -m peerreview.neohypo pci

    aws s3 sync s3://mecadoi-archives/batch/deposited "${MECADOI_PATH}"
    eeb run --rm flask python -m peerreview.mecadoi --input-dir "/app/mecadoi" --no-dry-run

    eeb run --rm flask python -m peerreview.summarization --no-dry-run
  }

  mark_peer_reviews_as_published() {
    # weekly
    log "updates publication status (find out if a preprint has been published in a journal)"
    eeb run --rm flask python -m peerreview.published # maybe run only once a week?
  }

  smarttag() {
    # daily

    log "smarttag specified collection of preprints"
    eeb run --rm flask python -m sdg.sdneo subject-collections --api eebapi
    eeb run --rm flask python -m sdg.sdneo refereed-preprints --api eebapi
  }

  import_SD() {
    # manually
    log "import source data public data. Logging to `log/sdg.sdneo-publicsearch.log`"
    eeb run --rm flask python -m sdg.sdneo PUBLICSEARCH --api sdapi
  }

  merge_graph() {
    log "generate merged graph"
    eeb run --rm flask python -m sdg.sd_processing

    log "neo4j graph data science"
    eeb run --rm flask python -m sdg.sd_gds

    log "advanced graph data science"
    eeb run --rm flask python -m sdg.algonet
  }

  precompute_graph() {
    log "build preprint peer review descriptor voc"
    cat sdg/build_preprint_review_descriptor_voc.cql | eeb run --rm neo4j cypher-shell -a bolt://neo4j:7687 -u neo4j -p $NEO_PASSWORD

    log "review service descriptors"
    cat sdg/describe_review_services.cql | eeb run --rm neo4j cypher-shell -a bolt://neo4j:7687 -u neo4j -p $NEO_PASSWORD

    log "precompute the graph used by front end"
    eeb run --rm flask python -m sdg.sd_precompute

    log "precompute pilot docmap graph"
    eeb run --rm flask python -m sdg.sd_prepare_docmap

    log "generate stats report"
    cat sdg/SD-stats.cql | eeb run --rm neo4j cypher-shell -a bolt://neo4j:7687 -u neo4j -p $NEO_PASSWORD
  }

  merge_and_precompute_graph() {
    merge_graph
    precompute_graph
  }

  update_close() {
    # always run at the end of a db update
    log "closing the update"
    cat sdg/update_close.cql | eeb run --rm neo4j cypher-shell -a bolt://neo4j:7687 -u neo4j -p $NEO_PASSWORD
  }


  log "starting docker"
  eeb up --build --detach flask

  local PERIODICITY=$1
  log "Running ${PERIODICITY} update"
  update_open # track update status

  if [ "$PERIODICITY" == "weekly" ]; then
    update_fulltext_biorxiv
  fi

  import_peer_reviews
  mark_peer_reviews_as_published
  smarttag
  merge_and_precompute_graph
  update_close

  log "done"
}


dump_local_neo4j(){
  local DUMP_NAME=$1
  log "[dumping neo4j] - start"
  log "  * stopping docker-compose"

  # Make sure you dont have your neo4j running:
  eeb down

  # rotate backup once before we dump, just in case some old files are still there
  rotate_backup

  log "  * setting right permissions to dir: dumps"
  sudo chown 7474:7474 dumps

  log "  * dumping to dumps/$DUMP_NAME"
  # dump the contents of your database using a temporary container
  docker run --rm --name neo4j-dump --env-file .env --mount type=bind,source=$PWD/data/neo4j-data,target=/data --mount type=bind,source=$PWD/dumps,target=/dumps neo4j:4.4 bin/neo4j-admin dump --database=neo4j --to=/dumps/$DUMP_NAME
  log "[dumping neo4j] - done"

  log "  * restarting docker-compose"
  eeb up --detach flask

}

deploy_database(){
  local SERVER_ADDRESS="ec2-user@$1"
  local NEO4J_DUMP_LOCAL_PATH=$2
  local NEO4J_DUMP_REMOTE_PATH="/home/ec2-user/sd-graph/neo4j.dump"

  log "[deploy database] - $SERVER_ADDRESS"

  log "  * scp $NEO4J_DUMP_LOCAL_PATH"
  scp $NEO4J_DUMP_LOCAL_PATH $SERVER_ADDRESS:$NEO4J_DUMP_REMOTE_PATH

  log "  * scp loading neo4j dump on remote"
  ssh $SERVER_ADDRESS "cd /home/ec2-user/sd-graph && ./deploy/__db__/remote"

  # TODO: can this wait be removed?
  local wait_for_docker_in_minutes=5
  log "  * waiting $wait_for_docker_in_minutes minutes to be sure docker is up and running"
  sleep $(( $wait_for_docker_in_minutes * 60 ))
}

rotate_backup(){
  # you may need to add something like this to your sudoers file
  #
  #     eeb ALL=(root) NOPASSWD: /bin/chown -R eeb\:eeb dumps
  #     eeb ALL=(root) NOPASSWD: /bin/chown 7474\:7474 dumps


  log "make sure all previously dumped databases files belong to our user"
  sudo /bin/chown -R `whoami`:`whoami` dumps

  log "  * rotating backups"
  logrotate deploy/__db__/logrotate-graph.db.dump.conf --state deploy/__db__/logrotate.state --force
}

cache_warmup(){
  local SERVER_ADDRESS="ec2-user@$1"
  ssh -tt $SERVER_ADDRESS "cd /home/ec2-user/sd-graph && ./deploy/cache_warmup --no-progress"
}

send_notifications(){
  log "sending notifications"

  # this file stores the time when the last notifications were sent. We need to send out notifications for all reviews posted after this point.
  file_last_notification="peerreview/last_notification.txt"
  date_format="--iso-8601=s"
  # if the file exists and has a valid date, use that. Otherwise use yesterday's date.
  after=$(date -f "${file_last_notification}" "${date_format}" || date --date "yesterday" "${date_format}")
  reviewed_by="review commons"
  recipient="${NOTIFICATIONS_RECIPIENT}"
  sender="${NOTIFICATIONS_SENDER}"
  eeb run --rm flask python -m peerreview.notify --after "${after}" --reviewed-by "${reviewed_by}" --recipient "${recipient}" --sender "${sender}" --no-dry-run
  # Update the time when the last notifications were sent to now.
  date "${date_format}" > "${file_last_notification}"
}

cleanup() {
  eeb down
}

main() {
  load_dotenv

  # parse arguments
  while [ $# -gt 0 ]; do
    case "$1" in
      --stage_name*|-s*)
        if [[ "$1" != *=* ]]; then shift; fi # Value is next arg if no `=`
        STAGE_NAME="${1#*=}"
        ;;
      --periodicity*|-p*)
        if [[ "$1" != *=* ]]; then shift; fi
        PERIODICITY="${1#*=}"
        ;;
      --log_dir*|-L*)
        if [[ "$1" != *=* ]]; then shift; fi
        LOG_DIR="${1#*=}"
        ;;
      --help|-h)
        echo "--------------------------------------------------------------------------------------"
        echo "deploy db"
        echo "--------------------------------------------------------------------------------------"
        echo ""
        echo "deploy/db --stage_name STAGE_NAME --periodicity PERIODICITY [--log_dir 'PATH/TO/LOG/DIRECTORY']"
        echo ""
        echo "Options:"
        echo "  --help        | -h"
        echo "  --stage_name  | -s (staging|production)"
        echo "  --periodicity | -p (daily|weekly)"
        echo "  --log_dir    | -l 'quoted/path/to/log/directory'"
        echo ""
        echo "example of usage:"
        echo "  deploy/db --stage_name staging --periodicity daily --log_dir '/home/eeb/logs'"
        echo ""
        exit 0
        ;;
      *)
        >&2 printf "Error: Invalid argument\n"
        exit 1
        ;;
    esac
    shift
  done

  case $STAGE_NAME in
    "staging")
      REMOTE_HOST="eeb-dev.embo.org"
      ;;
    "production")
      REMOTE_HOST="eeb.embo.org"
      ;;
    *)
      echo "[ERROR] Argument - please specify a valid --stage_name (staging|production)"
      exit 1
      ;;
  esac

  case $PERIODICITY in
    "daily"|"weekly")
    # valid periodicity, continue
    ;;
    *)
    echo "[ERROR] Argument - please specify a valid --periodicity (daily|weekly)"
    exit 1
    ;;
  esac

  if [ -z $LOG_DIR ]; then
    LOG_DIR="."
  fi
  local timestamp="$(date +%Y-%m-%d_%H-%M-%S)"
  LOG_FILE="${LOG_DIR}/db-${STAGE_NAME}-${PERIODICITY}-${timestamp}.log"

  # LOG_FILE is specified, let's redirect stdout and stderr to the $LOG_FILE
  exec >> $LOG_FILE
  exec 2>&1

  log "############################################################"
  log "##"
  log "## STARTING SD-GRAPH DB DEPLOY"
  log "##"
  local

  # flock creates an exclusive lock on $LOCK_FILE. Everything within the round brackets is only executed if the lock
  # could be acquired; if the lock is already held by another process, flock returns instantly due to --nonblock.
  # This mechanism prevents any concurrent db updates for the same stage.
  LOCK_FILE="/var/lock/eeb-db-${STAGE_NAME}"
  (
    if ! flock --nonblock 9; then
      echo "Failed to acquire lock. Is another db update still running?"
      exit 1
    fi

    update_local_neo4j $PERIODICITY
    NEO4J_DUMP_NAME="neo4j.dump"
    dump_local_neo4j $NEO4J_DUMP_NAME
    deploy_database $REMOTE_HOST dumps/$NEO4J_DUMP_NAME
    rotate_backup
    cache_warmup $REMOTE_HOST
    send_notifications
    cleanup
  ) 9>${LOCK_FILE}

  log "##"
  log "## FINISHED SD-GRAPH DB DEPLOY"
  log "##"
  log "############################################################"
  echo "done :)"
}

main $@
